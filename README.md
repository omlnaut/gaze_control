# gaze_control

The overall goal of this repository is to experiment with user interaction by gaze detection. For example looking up/down/right/left to replace arrow key input.

# Current step
- Load trained fastai model

Resources:
- https://towardsdatascience.com/video-streaming-in-the-jupyter-notebook-635bc5809e85
- https://github.com/maartenbreddels/ipywebrtc

# Next steps:
- Classify myself lookin in directions

# Done so far:
- Take single images from the stream
- Get a webcam stream going in a jupyter notebook.
- push from paperspace to github
- cloned on paperspace
- setup repo on github